# Guia de Web Scraping para Knowledge Base

**Vers√£o:** 1.0.0
**Data:** 16 de novembro de 2025
**Prop√≥sito:** Extrair documenta√ß√£o t√©cnica de sites e popular a knowledge base do RAG

---

## üìã Vis√£o Geral

Este guia documenta como criar scrapers para extrair artigos de sites de documenta√ß√£o t√©cnica e convert√™-los em arquivos Markdown (.md) formatados para o sistema RAG do BidAnalyzee.

---

## üìÑ Formato dos Arquivos .md

### Estrutura com Frontmatter YAML

Todos os arquivos .md gerados pelo scraper **devem** incluir frontmatter YAML com os seguintes campos:

```markdown
---
title: "T√≠tulo Completo do Artigo"
url: "https://docs.exemplo.com/artigos/caminho-completo"
source: "Nome do Site de Documenta√ß√£o"
category: "Hardware"
date: "2025-11-16"
---

# Conte√∫do do artigo em Markdown

Texto do artigo aqui...
```

### Campos Obrigat√≥rios

| Campo | Descri√ß√£o | Exemplo |
|-------|-----------|---------|
| **title** | T√≠tulo completo do artigo | `"Especifica√ß√µes T√©cnicas - Processadores Intel Xeon"` |
| **url** | URL completa da p√°gina original | `"https://docs.intel.com/processors/xeon-gold-specs"` |
| **source** | Nome da fonte/site | `"Intel ARK - Product Specifications"` |
| **category** | **Categoria do conte√∫do (OBRIGAT√ìRIO)** | `"Hardware"`, `"Software"`, `"Legisla√ß√£o"` |
| **date** | Data da extra√ß√£o (YYYY-MM-DD) | `"2025-11-16"` |

**IMPORTANTE sobre `category`:**
- Este campo √© **obrigat√≥rio** e ser√° usado na coluna "Categoria" do CSV de an√°lise
- O scraper deve determinar a categoria do artigo baseado no site/se√ß√£o de origem
- A categoria aparecer√° no CSV final de conformidade

**Categorias Comuns:**
- `"Hardware"` - Especifica√ß√µes de equipamentos, servidores, componentes
- `"Software"` - Requisitos de software, licen√ßas, APIs
- `"Legisla√ß√£o"` - Leis, decretos, portarias
- `"Normas T√©cnicas"` - NBR, ISO, IEC, ABNT
- `"Certifica√ß√µes"` - ANATEL, INMETRO, ISO
- `"Seguran√ßa"` - Protocolos de seguran√ßa, criptografia
- `"Redes"` - Topologias, protocolos de rede
- `"Qualifica√ß√£o"` - Documenta√ß√£o, atestados t√©cnicos

### Campos Opcionais

| Campo | Descri√ß√£o | Exemplo |
|-------|-----------|---------|
| **author** | Autor do artigo (se dispon√≠vel) | `"Intel Corporation"` |
| **tags** | Tags/palavras-chave (separadas por v√≠rgula) | `"processador, xeon, servidor, datacenter"` |
| **version** | Vers√£o da documenta√ß√£o (se aplic√°vel) | `"v3.2"` |
| **last_updated** | Data de √∫ltima atualiza√ß√£o no site original | `"2025-10-15"` |

---

## üîß Como o Sistema Usa essas Informa√ß√µes

### 1. **Durante a Indexa√ß√£o (RAG Ingestion)**

O `ingestion_pipeline.py` extrai automaticamente o frontmatter:

```python
# Extrai metadata do frontmatter
frontmatter, content = self._extract_frontmatter(raw_content)

doc = {
    "filename": file_path.name,
    "content": content,  # Sem frontmatter
    "title": frontmatter.get("title", file_path.stem),
    "url": frontmatter.get("url", ""),
    "source": frontmatter.get("source", ""),
    "category": frontmatter.get("category", ""),  # OBRIGAT√ìRIO
    "date": frontmatter.get("date", "")
}
```

### 2. **Durante a Busca (RAG Search)**

O `rag_search.py` retorna o metadata com cada resultado:

```json
{
  "query": "processador intel xeon",
  "results": [
    {
      "text": "Processadores Intel Xeon Gold 6XXX ou superior...",
      "similarity_score": 0.92,
      "metadata": {
        "title": "Especifica√ß√µes T√©cnicas - Processadores",
        "url": "https://docs.intel.com/processors/xeon",
        "category": "Hardware",
        "filename": "intel_xeon_specs.md",
        "chunk_index": 5
      }
    }
  ]
}
```

### 3. **No CSV de An√°lise**

As colunas `Categoria`, `Fonte_Titulo` e `Fonte_URL` s√£o preenchidas automaticamente do RAG:

```csv
ID,Requisito,Categoria,Veredicto,Confian√ßa,Evid√™ncias,Racioc√≠nio,Recomenda√ß√µes,Fonte_Titulo,Fonte_URL
1,"Processador Intel Xeon...",Hardware,CONFORME,0.95,"...","...","...","Especifica√ß√µes T√©cnicas - Processadores","https://docs.intel.com/processors/xeon"
```

**Observa√ß√£o:** A coluna `Categoria` vem do campo `category` do frontmatter do documento scraped.
Este √© o principal motivo pelo qual `category` √© obrigat√≥rio no scraper.

---

## üï∑Ô∏è Template de Web Scraper

### Exemplo B√°sico (Python + BeautifulSoup)

```python
#!/usr/bin/env python3
"""
Web Scraper para [Nome do Site de Documenta√ß√£o]

Extrai artigos t√©cnicos e converte para formato .md com frontmatter
compat√≠vel com o sistema RAG do BidAnalyzee.
"""

import requests
from bs4 import BeautifulSoup
from pathlib import Path
from datetime import datetime
import re
import time


class TechDocsScraper:
    """Scraper para documenta√ß√£o t√©cnica"""

    def __init__(self, base_url: str, output_dir: str):
        self.base_url = base_url
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Headers para evitar bloqueio
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; BidAnalyzee/1.0; +http://exemplo.com)'
        }

    def slugify(self, text: str) -> str:
        """Converte t√≠tulo em nome de arquivo v√°lido"""
        text = text.lower()
        text = re.sub(r'[^\w\s-]', '', text)
        text = re.sub(r'[-\s]+', '-', text)
        return text[:100]  # Limita tamanho

    def fetch_page(self, url: str) -> BeautifulSoup:
        """Busca e parseia uma p√°gina"""
        print(f"üì• Fetching: {url}")
        response = requests.get(url, headers=self.headers, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'

        return BeautifulSoup(response.text, 'html.parser')

    def extract_article(self, soup: BeautifulSoup, url: str) -> dict:
        """
        Extrai informa√ß√µes do artigo

        IMPORTANTE: Adapte os seletores CSS para o site espec√≠fico!
        """
        # EXEMPLO - Ajustar para o site real
        title = soup.find('h1', class_='article-title')
        title_text = title.get_text().strip() if title else "Sem T√≠tulo"

        # Corpo do artigo
        content_div = soup.find('div', class_='article-content')
        if not content_div:
            content_div = soup.find('article')

        if not content_div:
            raise ValueError("N√£o foi poss√≠vel encontrar o conte√∫do do artigo")

        # Converter HTML para Markdown (simples)
        markdown_content = self.html_to_markdown(content_div)

        # Metadata
        source_name = soup.find('meta', {'property': 'og:site_name'})
        source_text = source_name.get('content') if source_name else "Documenta√ß√£o T√©cnica"

        # IMPORTANTE: Detectar categoria baseada no site/URL
        category = self.detect_category(url, soup)

        return {
            'title': title_text,
            'url': url,
            'source': source_text,
            'category': category,  # OBRIGAT√ìRIO
            'content': markdown_content,
            'date': datetime.now().strftime('%Y-%m-%d')
        }

    def detect_category(self, url: str, soup: BeautifulSoup) -> str:
        """
        Detecta a categoria do artigo baseado na URL ou conte√∫do

        IMPORTANTE: Adapte esta l√≥gica para cada site espec√≠fico!

        Args:
            url: URL da p√°gina
            soup: BeautifulSoup da p√°gina

        Returns:
            Categoria do documento
        """
        # Estrat√©gia 1: Detectar pela URL
        if '/hardware/' in url or '/processors/' in url or '/servers/' in url:
            return "Hardware"
        elif '/software/' in url or '/apps/' in url or '/api/' in url:
            return "Software"
        elif '/law/' in url or '/legislation/' in url or '/legal/' in url:
            return "Legisla√ß√£o"
        elif '/standards/' in url or '/norms/' in url or '/iso/' in url:
            return "Normas T√©cnicas"
        elif '/certification/' in url or '/compliance/' in url:
            return "Certifica√ß√µes"

        # Estrat√©gia 2: Detectar por tags HTML (se o site usar)
        category_tag = soup.find('meta', {'name': 'category'})
        if category_tag:
            return category_tag.get('content', 'Geral')

        # Estrat√©gia 3: Detectar por breadcrumb
        breadcrumb = soup.find('nav', {'aria-label': 'breadcrumb'})
        if breadcrumb:
            links = breadcrumb.find_all('a')
            if len(links) > 1:
                # Pega segunda categoria do breadcrumb
                second_category = links[1].get_text().strip()
                return second_category

        # Default: tente inferir do t√≠tulo ou retorne "Geral"
        return "Geral"

    def html_to_markdown(self, element) -> str:
        """
        Converte HTML para Markdown b√°sico

        Para convers√£o mais robusta, use bibliotecas como:
        - html2text
        - markdownify
        - pypandoc
        """
        # EXEMPLO SIMPLES - Use biblioteca adequada em produ√ß√£o
        markdown = []

        for tag in element.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'ul', 'ol', 'pre', 'code']):
            if tag.name == 'h1':
                markdown.append(f"# {tag.get_text().strip()}\n")
            elif tag.name == 'h2':
                markdown.append(f"## {tag.get_text().strip()}\n")
            elif tag.name == 'h3':
                markdown.append(f"### {tag.get_text().strip()}\n")
            elif tag.name == 'h4':
                markdown.append(f"#### {tag.get_text().strip()}\n")
            elif tag.name == 'p':
                markdown.append(f"{tag.get_text().strip()}\n")
            elif tag.name == 'ul':
                for li in tag.find_all('li', recursive=False):
                    markdown.append(f"- {li.get_text().strip()}")
                markdown.append("")
            elif tag.name == 'ol':
                for i, li in enumerate(tag.find_all('li', recursive=False), 1):
                    markdown.append(f"{i}. {li.get_text().strip()}")
                markdown.append("")
            elif tag.name in ['pre', 'code']:
                code_text = tag.get_text().strip()
                markdown.append(f"```\n{code_text}\n```\n")

        return "\n".join(markdown)

    def save_markdown(self, article: dict):
        """Salva artigo como arquivo .md com frontmatter"""
        # Nome do arquivo
        filename = f"{self.slugify(article['title'])}.md"
        filepath = self.output_dir / filename

        # Frontmatter YAML
        frontmatter = f"""---
title: "{article['title']}"
url: "{article['url']}"
source: "{article['source']}"
category: "{article['category']}"
date: "{article['date']}"
---

"""

        # Conte√∫do completo
        full_content = frontmatter + article['content']

        # Salvar
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(full_content)

        print(f"‚úÖ Saved: {filename}")
        return filepath

    def scrape_article(self, url: str):
        """Scrape um √∫nico artigo"""
        try:
            soup = self.fetch_page(url)
            article = self.extract_article(soup, url)
            filepath = self.save_markdown(article)

            # Rate limiting - ser educado com o servidor
            time.sleep(2)

            return filepath

        except Exception as e:
            print(f"‚ùå Error scraping {url}: {e}")
            return None

    def scrape_index(self, index_url: str, link_selector: str):
        """
        Scrape m√∫ltiplos artigos de uma p√°gina √≠ndice

        Args:
            index_url: URL da p√°gina com lista de artigos
            link_selector: Seletor CSS para links dos artigos
        """
        soup = self.fetch_page(index_url)
        links = soup.select(link_selector)

        print(f"\nüìö Found {len(links)} articles to scrape\n")

        for i, link in enumerate(links, 1):
            href = link.get('href')
            if not href.startswith('http'):
                href = self.base_url.rstrip('/') + '/' + href.lstrip('/')

            print(f"\n[{i}/{len(links)}] Processing: {href}")
            self.scrape_article(href)

            # Rate limiting entre artigos
            time.sleep(3)

        print(f"\n‚úÖ Scraping completed! Files saved to: {self.output_dir}")


# Exemplo de uso
if __name__ == "__main__":
    # Configura√ß√£o - AJUSTAR PARA O SITE REAL
    scraper = TechDocsScraper(
        base_url="https://docs.exemplo.com",
        output_dir="data/knowledge_base/producao"
    )

    # Op√ß√£o 1: Scrape um artigo √∫nico
    scraper.scrape_article("https://docs.exemplo.com/artigos/processadores-xeon")

    # Op√ß√£o 2: Scrape m√∫ltiplos artigos de um √≠ndice
    scraper.scrape_index(
        index_url="https://docs.exemplo.com/hardware/indice",
        link_selector="div.article-list a.article-link"  # Ajustar seletor
    )
```

---

## üéØ Sites Alvo Recomendados

Para documenta√ß√£o de licita√ß√µes p√∫blicas brasileiras:

| Site | Conte√∫do | Prioridade |
|------|----------|-----------|
| **Portal da Transpar√™ncia** | Leis, decretos, portarias | ‚≠ê‚≠ê‚≠ê |
| **Planalto (LegisWeb)** | Lei 8.666/93, Lei 14.133/2021 | ‚≠ê‚≠ê‚≠ê |
| **TCU - Tribunal de Contas da Uni√£o** | Ac√≥rd√£os, s√∫mulas, jurisprud√™ncia | ‚≠ê‚≠ê‚≠ê |
| **INMETRO** | Normas t√©cnicas, certifica√ß√µes | ‚≠ê‚≠ê |
| **ANATEL** | Regulamentos telecomunica√ß√µes | ‚≠ê‚≠ê |
| **ABNT** | Normas t√©cnicas brasileiras | ‚≠ê‚≠ê |

---

## ‚öôÔ∏è Bibliotecas Recomendadas

### Para Scraping

```bash
pip install requests beautifulsoup4 lxml
```

- `requests` - HTTP requests
- `beautifulsoup4` - HTML parsing
- `lxml` - Parser r√°pido

### Para Convers√£o HTML ‚Üí Markdown

```bash
pip install html2text markdownify
```

**Exemplo com html2text:**
```python
import html2text

h = html2text.HTML2Text()
h.ignore_links = False
h.ignore_images = False
markdown = h.handle(html_content)
```

### Para Sites JavaScript-heavy

```bash
pip install selenium playwright
```

Se o site usa JavaScript para renderizar conte√∫do, use Selenium ou Playwright.

---

## üìä Depois de Scraping - Indexar no RAG

Ap√≥s extrair os artigos:

```bash
# 1. Verificar arquivos gerados
ls -lh data/knowledge_base/producao/*.md

# 2. Indexar no RAG
python scripts/index_knowledge_base.py

# 3. Testar busca
python scripts/rag_search.py --requirement "processador intel xeon" --top-k 5
```

---

## ‚ö†Ô∏è Considera√ß√µes Legais e √âticas

### ‚úÖ Permitido:
- Documenta√ß√£o p√∫blica (leis, normas governamentais)
- Sites com termos de uso permitindo scraping educacional/pesquisa
- Conte√∫do com licen√ßas abertas (CC-BY, etc.)

### ‚ùå Evitar:
- Sites com `robots.txt` bloqueando scraping
- Conte√∫do protegido por paywall
- Rate muito alto (pode derrubar o servidor)
- Conte√∫do protegido por direitos autorais sem permiss√£o

### üõ°Ô∏è Boas Pr√°ticas:
- Respeite `robots.txt`
- Use User-Agent identific√°vel
- Implemente rate limiting (2-3 segundos entre requests)
- Fa√ßa cache local (n√£o re-scrape desnecessariamente)
- Entre em contato com o site se for scraping massivo

---

## üß™ Testando o Scraper

```python
# test_scraper.py
def test_scraper():
    scraper = TechDocsScraper(
        base_url="https://docs.exemplo.com",
        output_dir="data/knowledge_base/test"
    )

    # Teste com 1 artigo
    filepath = scraper.scrape_article("https://docs.exemplo.com/teste")

    # Verificar frontmatter
    with open(filepath, 'r') as f:
        content = f.read()
        assert content.startswith('---')
        assert 'title:' in content
        assert 'url:' in content
        assert 'source:' in content

    print("‚úÖ Scraper test passed!")

if __name__ == "__main__":
    test_scraper()
```

---

## üìö Pr√≥ximos Passos

1. **Identifique os 2 sites alvo** que voc√™ quer scraper
2. **Inspecione a estrutura HTML** (DevTools do browser)
3. **Adapte o template acima** com os seletores CSS corretos
4. **Teste com 1 artigo** primeiro
5. **Escale para m√∫ltiplos artigos**
6. **Indexe no RAG** e teste a busca

---

## üÜò Troubleshooting

### Erro: "N√£o encontrou o conte√∫do"
- Verifique os seletores CSS (`soup.find(...)`)
- Use DevTools do navegador para inspecionar HTML
- Site pode usar JavaScript ‚Üí use Selenium/Playwright

### Erro: "Connection timeout"
- Aumente `timeout` em `requests.get(..., timeout=60)`
- Verifique se site n√£o est√° bloqueando bot

### Erro: "403 Forbidden"
- Site est√° bloqueando bots
- Ajuste User-Agent header
- Adicione cookies/session se necess√°rio

### Frontmatter n√£o reconhecido
- Verifique formato YAML (`:` com espa√ßo depois)
- Teste com regex: `^---\s*\n(.*?)\n---\s*\n`

---

**Boa sorte com o scraping! üï∑Ô∏è**

Lembre-se: seja educado com os servidores alheios e respeite os termos de uso.
