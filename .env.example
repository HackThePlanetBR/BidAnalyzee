# BidAnalyzee - Environment Variables
# Copy this file to .env and fill in your actual values
# NEVER commit .env to version control!

# ============================================
# PINECONE CONFIGURATION
# ============================================
# STATUS: Account needs to be created (see docs/PINECONE_SETUP.md)
# Follow the setup guide to create your free account and get these values

# Your Pinecone API key (get from: https://app.pinecone.io/)
PINECONE_API_KEY=your_pinecone_api_key_here

# Your Pinecone environment (e.g., us-west1-gcp, eu-west1-gcp)
PINECONE_ENVIRONMENT=your_pinecone_environment_here

# Your Pinecone index name
PINECONE_INDEX_NAME=bidanalyzee-knowledge-base

# Index configuration (for when you create it)
PINECONE_DIMENSION=1536
PINECONE_METRIC=cosine

# ============================================
# N8N CONFIGURATION
# ============================================
# URL of your n8n instance (self-hosted or cloud)
# Your instance: Self-hosted (Docker) at https://hacktheplanet.net.br/
N8N_BASE_URL=https://hacktheplanet.net.br

# n8n webhook URL for the query service (from HistÃ³ria 3.2)
# Update the path after creating the webhook workflow
N8N_QUERY_SERVICE_URL=https://hacktheplanet.net.br/webhook/query

# n8n API key (if using authenticated endpoints)
# Generate this from your n8n instance: Settings > API
N8N_API_KEY=your_n8n_api_key_here

# ============================================
# EMBEDDING MODEL CONFIGURATION
# ============================================
# Embedding model to use (default: llama-text-embed-v2)
EMBEDDING_MODEL=llama-text-embed-v2

# Embedding API endpoint (if using external service)
EMBEDDING_API_URL=https://api.example.com/embeddings

# Embedding API key (if required)
EMBEDDING_API_KEY=your_embedding_api_key_here

# ============================================
# TECHNICAL ANALYST - RAG CONFIGURATION (Sprint 5)
# ============================================
# Vector Store Configuration
RAG_VECTOR_STORE=faiss                    # faiss (local) | pinecone (cloud)
RAG_FAISS_INDEX_PATH=data/vector_store/faiss

# Embeddings Configuration
RAG_EMBEDDINGS_PROVIDER=local             # local (sentence-transformers) | openai
RAG_EMBEDDINGS_MODEL=all-MiniLM-L6-v2     # Model name
RAG_EMBEDDINGS_DIMENSION=384              # Embedding dimension

# OpenAI Configuration (Future - when migrating to cloud)
# OPENAI_API_KEY=sk-...
# OPENAI_EMBEDDINGS_MODEL=text-embedding-3-small
# OPENAI_EMBEDDINGS_DIMENSION=1536

# Knowledge Base Configuration
RAG_KNOWLEDGE_BASE_PATH=data/knowledge_base/mock
RAG_CHUNK_SIZE=1000                       # Characters per chunk
RAG_CHUNK_OVERLAP=200                     # Overlap between chunks

# Search Configuration
RAG_TOP_K=5                               # Number of results to return
RAG_SIMILARITY_THRESHOLD=0.7              # Minimum similarity score (0.0-1.0)

# ============================================
# WEB SCRAPERS CONFIGURATION
# ============================================
# Selenium Configuration
SCRAPERS_USE_SELENIUM=true                # Enable Selenium for Compliance and TechDocs
SCRAPERS_HEADLESS=true                    # Run browser in headless mode

# Proxy Configuration
SCRAPERS_USE_PROXY=false                  # Enable proxy for Selenium
SCRAPERS_PROXY_URL=                       # Proxy URL (e.g., http://proxy.example.com:8080)
# If empty and USE_PROXY=true, will auto-detect from HTTP_PROXY environment variable

# Rate Limiting
SCRAPERS_DELAY_BETWEEN_REQUESTS=1.5       # Seconds between requests (be polite!)

# Output Configuration
SCRAPERS_OUTPUT_DIR=data/knowledge_base/genetec

# Browser Configuration
SCRAPERS_CHROME_BINARY_PATH=              # Path to Chrome/Chromium (auto-detect if empty)
SCRAPERS_CHROMEDRIVER_PATH=               # Path to ChromeDriver (auto-detect if empty)

# ============================================
# SYSTEM CONFIGURATION
# ============================================
# Confidence threshold for conformity analysis (NFR2: > 85%)
CONFIDENCE_THRESHOLD=0.85

# Number of results to retrieve from Pinecone (before re-ranking)
TOP_K_RESULTS=20

# Number of results to keep after re-ranking
RERANK_TOP_N=5

# Maximum number of LOOP iterations before forcing HALT (SHIELD)
MAX_LOOP_ITERATIONS=3

# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# ============================================
# DATA SOURCE CONFIGURATION (for n8n ingestion)
# ============================================
# Base URL of the technical documentation portal
# STATUS: Public access (no authentication required)
# NOTE: No API available - web scraping will be used
TECHDOCS_BASE_URL=https://techdocs.genetec.com

# Scraping interval (in days) - default: 120 days (4 months)
SCRAPING_INTERVAL_DAYS=120

# Scraping configuration
# Since there's no API, we'll implement rate limiting on our side
SCRAPING_DELAY_SECONDS=2
SCRAPING_MAX_CONCURRENT_REQUESTS=3

# Google Sheets ID for URL tracking (MVP only)
GOOGLE_SHEETS_ID=your_google_sheets_id_here

# Google Sheets API credentials path (JSON file)
GOOGLE_CREDENTIALS_PATH=./credentials/google_credentials.json

# ============================================
# OPTIONAL: CLAUDE API (if needed for embeddings or custom models)
# ============================================
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# ============================================
# PROJECT PATHS (usually no need to change)
# ============================================
# Root directory for data storage
DATA_ROOT=./data

# Directory for analyses
ANALYSES_DIR=./data/analyses

# Directory for state files
STATE_DIR=./data/state

# Directory for templates
TEMPLATES_DIR=./data/templates

# ============================================
# DEVELOPMENT / DEBUG
# ============================================
# Enable detailed logging of SHIELD phases (true/false)
DEBUG_SHIELD=false

# Enable token usage logging (true/false)
LOG_TOKEN_USAGE=true

# Dry run mode (simulates operations without executing - for testing)
DRY_RUN=false
